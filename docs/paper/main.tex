\documentclass[logo]{dsme}
% \documentclass[twocolumn]{article}

\usepackage[authoryear, sort&compress, round]{natbib}
\bibliographystyle{abbrvnat}

%% Title
\title{\mpx{}: Mixed Precision Training for JAX}

\usepackage{authblk}

\author[1]{Alexander Gräfe}
\author[1]{Sebastian Trimpe}

\renewcommand{\shortauthors}{Gräfe et al.}

\affil[1]{Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University}

\codelabel{\faGithub} % defaults to "Code:"
\codeurl{https://github.com/Data-Science-in-Mechanical-Engineering/mixed_precision_for_JAX}
\codeurlshort{mixed\_precision\_for\_JAX}

% \date{}

% # build: python3 -m build --sdist
% # upload: twine upload --repository pypi dist/* 

% user packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mdframed}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\usepackage{caption}
\usepackage{subcaption}
\captionsetup[subfigure]{justification=centering}

\usepackage{siunitx}

\DeclareCaptionType[fileext=los,placement={!ht}]{example}

\newcommand{\mpx}{\textsc{MPX}}
\newcommand{\capt}[1]{\mdseries{\emph{#1}}}

\begin{document}
\sloppy
\maketitle
\begin{abstract}
Mixed-precision training has emerged as an indispensable tool for enhancing the efficiency of neural network training in recent years. 
    Concurrently, JAX has grown in popularity as a versatile machine learning toolbox.
    However, it currently lacks robust support for mixed-precision training.
    We propose MPX, a mixed-precision training toolbox for JAX that simplifies and accelerates the training of large-scale neural networks while preserving model accuracy. 
    MPX seamlessly integrates with popular toolboxes such as Equinox and Flax, allowing users to convert full-precision pipelines to mixed-precision versions with minimal modifications. 
    By casting both inputs and outputs to half precision, and introducing a dynamic loss-scaling mechanism, MPX alleviates issues like gradient underflow and overflow that commonly arise in half precision computations. 
    Its design inherits critical features from JAX's type-promotion behavior, ensuring that operations take place in the correct precision and allowing for selective enforcement of full precision where needed (e.g., sums, means, or softmax). 
    MPX further provides wrappers for automatic creation and management of mixed-precision gradients and optimizers, enabling straightforward integration into existing JAX training pipelines.
    \mpx{}'s source code, documentation, and usage examples are available at \url{github.com/Data-Science-in-Mechanical-Engineering/mixed_precision_for_JAX}.
\end{abstract}

\section{Introduction}

In recent years, JAX~\citep{deepmind2020jax} has emerged as a popular tool for training neural networks. 
A standout feature of JAX is its just-in-time compilation, which significantly accelerates code execution on hardware accelerators such as GPUs and TPUs. 
Additionally, JAX supports scalable multi-device and multi-node training across computing clusters through user-friendly and efficient sharding mechanisms.

To extend JAX's capabilities, several libraries have been developed, including Equinox, Flax, and Optax~\citep{kidger2021equinox,flax2020github,deepmind2020jax}. 
Despite these advancements, one critical area where JAX falls short is in supporting mixed-precision training, a method essential for efficiently handling large models~\citep{mixed_precision_paper}. 
Although JMP~\citep{jmp} was previously available for this purpose, it is no longer maintained and cannot process arbitrary JAX PyTrees commonly used in modern neural-network toolboxes.

Mixed-precision training plays a pivotal role in optimizing the performance of large model training by executing both forward and backward passes in half-precision floating-point format (\qty{16}{\bit}). 
This technique significantly reduces memory requirements for intermediate activations and speeds up training by minimizing memory transfer overheads while utilizing specialized half-precision tensor cores (when supported by hardware), all without compromising accuracy~\citep{mixed_precision_paper}.

To facilitate the straightforward implementation of mixed-precision training in JAX, we introduce \mpx{}, a mixed-precision training toolbox tailored for JAX. 
\mpx{} builds upon the JMP implementation and leverages Equinox's flexibility to overcome limitations associated with handling arbitrary PyTrees. 
It further introduces gradient transformations and optimizer wrappers that enable seamless conversion from a full-precision pipeline to mixed precision with minimal modifications. The source code, documentation, and usage examples for \mpx{} are accessible at \url{github.com/Data-Science-in-Mechanical-Engineering/mixed_precision_for_JAX}.

Before explaining the core functionality of \mpx{}, we provide an overview of mixed-precision training principles. 
Subsequently, we demonstrate how \mpx{} can be integrated into existing workflows through sample code illustrating its main features.
Finally, we evaluate the performance of \mpx{} in the context of training vision transformers.

\section{Basics of mixed precision Training}
In this section, we summarize the foundational principles of mixed precision training as outlined in the original method~\citep{mixed_precision_paper}. 
Mixed precision training involves executing most computations during the forward and backward passes of a neural network using 16-bit floating-point numbers. 
This approach offers two primary advantages: 
\begin{itemize} 
    \item Reduced GPU Memory Usage: Mixed precision can decrease memory usage by approximately \SI{50}{\percent} compared to full precision. 
    \item Potential Speedup: This technique can lead to faster execution due to reduced memory access times and the utilization of specialized half-precision tensor cores available in TPUs or GPUs. 
\end{itemize}

\subsection{Loss Scaling}
One downside of using half-precision floating-point numbers is their decreased resolution. 
Consequently, small gradients may be rounded to zero, which can negatively impact training performance.

To mitigate these effects, loss scaling has been proposed. 
The main idea involves multiplying the loss by a factor greater than one and calculating the gradient of this scaled loss. 
Consequently, the gradients are also scaled by this factor, making them larger and allowing them to be represented more accurately by half-precision numbers.

However, selecting a constant scaling factor poses challenges. 
A small scaling factor might still result in gradients being rounded to zero, while an excessively large scaling factor can cause overflows where gradients become infinite. 
Additionally, different stages of training may benefit from varying scaling factors depending on the training state.

Dynamic loss scaling addresses this issue by adapting the scaling factor at runtime.
Its objective is to maximize the scaling factor for best accuracy with half-precision numbers while avoiding overflow. 
This is achieved through a simple heuristic that periodically increases the scaling factor and decreases it when gradients turn infinite.

In summary, mixed precision training with dynamic loss scaling involves the following steps:
\begin{enumerate}
    \item Calculate the prediction of the model in half-precision.
    \item Calculate the loss and scale it by a factor greater than 1.
    \item Calculate gradients of the scaled loss with respect to the model parameters.
    \item Converting gradients to float32.
    \item Divide them by the scaling factor to retrieve original gradients.
    \item Update the scaling factor:
    \begin{enumerate}[label=(\alph*)]
        \item If scaled gradients exceed float16's representable range (resulting in infinity), reduce the scaling and skip updating model parameters. 
        \item If scaled gradients remain within float16 range for an extended period, increase scaling.
    \end{enumerate}
    \item Update the model parameters.
\end{enumerate}

The primary goal of \mpx{} is to offer functionalities that streamline the implementation of all these steps in mixed precision training. 
In the following section, we provide an overview of these features.

\section{Implementation Details}

The \mpx{} library offers essential transformations for mixed precision training while preserving JAX's low-level flexibility. 
Similar to JMP~\citep{jmp}, our implementation's design leverages JAX's type-promotion behavior\footnote{\url{https://docs.jax.dev/en/latest/jep/9407-type-promotion.html}}, ensuring that operations are executed in the precision to which inputs and outputs have been cast—provided all constants in the function are positioned on the left side of the type-promotion lattice. 
Consequently, \mpx{} primarily focuses on casting function inputs and outputs to the desired precision.

Specifically, \mpx{} first defines functions for casting PyTrees (Section~\ref{sec:castingpytrees}), then utilizes these functions to cast both general functions (Section~\ref{sec:castingfunctions}) and gradient calculations (Section~\ref{sec:castinggradients}) into mixed precision. 
The gradient calculations include dynamic loss scaling (Section~\ref{sec:scaling}) to stabilize mixed precision training.
Moreover, \mpx{} provides wrappers around optimizers to exclude update steps where gradients are infinite (Section~\ref{sec:optimizer}).

\subsection{Transformations to Cast PyTrees}
\label{sec:castingpytrees}

\mpx{} provides several functions for casting arbitrary PyTrees to a desired floating-point data type. These functions include:
\texttt{cast\_tree(tree, dtype)}, \texttt{cast\_to\_half\_precision(x)}, \texttt{cast\_to\_float16(x)}, \texttt{cast\_to\_bfloat16(x)}, and \texttt{cast\_to\_float32(x)}. 
Each function inspects the leaves of the input PyTree, casting any leaf that is a JAX array with a floating-point type to the desired data type. 
Other leaves, such as integer arrays, remain unchanged. 
Excluding integer arrays is crucial to prevent accidental casting of random generator keys.

\subsection{Transformations to Cast Functions}
\label{sec:castingfunctions}

For casting entire functions, \mpx{} provides the transformation \texttt{cast\_function(func, dtype, return\_dtype=None)}. 
This transformation returns a new function that first applies the specified input data type to all inputs (via \texttt{cast\_tree(tree, dtype)}), then calls the original function, and finally casts the outputs to the specified (optional) return data type. 
Furthermore, \mpx{} includes \texttt{force\_full\_precision(func, return\_dtype)}, which ensures computations are carried out in full precision. 
This feature is particularly important for operations that are prone to overflow in \texttt{float16}.


\subsection{Automatic Loss Scaling}
\label{sec:scaling}

To facilitate the calculation of gradients via loss scaling, \mpx{} offers the class \texttt{DynamicLossScaling}, which extends \texttt{jmp.DynamicLossScaling}. 
This class manages dynamic loss scaling through the following methods:

\begin{itemize}
    \item \texttt{scale(x)}: Multiplies all floating-point leaves of the PyTree \texttt{x} by the current scaling factor.
    \item \texttt{unscale(x)}: Divides all floating-point leaves of \texttt{x} by the scaling factor and casts them to full precision.
    \item \texttt{adjust(grads\_finite)}: Updates the scaling factor based on the stability of the gradients, following the techniques described by~\cite{mixed_precision_paper}.
\end{itemize}

Because \texttt{DynamicLossScaling} inherits from \texttt{eqx.Module}, it is itself a PyTree. 
Consequently, it can be used within \texttt{jit}-compiled functions and also be used for multi-device and multi-node training using replicated sharding.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[t]{0.99\textwidth}
%         \centering
%         \includegraphics[width=0.99\textwidth]{assets/EQXOverview.pdf}
%         \caption{Full Precision Training via Equinox}
%         \vspace{0.2cm}
%     \end{subfigure}\hspace{0.1cm}
%     \begin{subfigure}[t]{0.99\textwidth}
%         \centering
%         \includegraphics[width=0.99\textwidth]{assets/MPXOverview.pdf}
%         \caption{Half Precision Training via Equinox and \mpx{}}
%     \end{subfigure}\hspace{0.1cm}
%     \caption{Transformation of training pipelines via MPX. \capt{An existing training pipeline (Top figure) can be transformed, by applying \texttt{@mpx.filter\_grad} to the forward pass. It automatically casts all inputs to half precision and calculates the gradient via dynamic loss scaling and \texttt{eqx.filter\_grad}.}}
%     \label{fig:overview}
% \end{figure}

\subsection{Transformations to Cast Gradient Calculation}
\label{sec:castinggradients}

\mpx{} provides Equinox-compatible versions of gradient-calculation functions: \texttt{filter\_grad(func, scaling, has\_aux=False, use\_mixed\_precision=True)} and \texttt{filter\_value\_and\_grad(func, scaling, has\_aux=False, use\_mixed\_precision=True)}. 
These functions compute gradients using mixed precision and loss scaling (as described in Section\ref{sec:scaling}) while automatically adjusting the loss scaling value. 
They act as drop-in replacements for the corresponding Equinox functions, enabling users to reuse existing training pipelines with minimal modifications (cf. Section\ref{sec:eqxvsmpxmodelupdate}).

To use these functions, the model's forward pass and loss computation should be combined into a single function, which is then passed to \texttt{mpx.filter\_value\_and\_grad} in place of \texttt{eqx.filter\_value\_and\_grad}. 
The transformed function then operates as follows (Figure~\ref{fig:overview}):

\begin{enumerate}
    \item Cast all input arguments to half precision (\texttt{float16} or \texttt{bfloat16}).
    \item Execute the original function (model forward pass and loss).
    \item Scale the function's output by the current dynamic scaling factor.
    \item Apply \texttt{equinox.filter\_grad} to obtain scaled gradients.
    \item Unscale the gradients by dividing them by the scaling factor.
    \item Check whether the gradients are finite.
    \item Update the loss scaling based on the finiteness of gradients.
    \item Return a tuple containing:
    \begin{itemize}
        \item The updated \texttt{scaling} object.
        \item A boolean indicating if the gradients are finite (for the optimizer step).
        \item The computed gradients, whose format is a PyTree with the same format as the model parameters.
        \item Auxiliary values (if \texttt{has\_aux=True}).
    \end{itemize}
\end{enumerate}



\subsection{Optimizer}
\label{sec:optimizer}

Since \mpx{} produces gradients as PyTrees, they are directly compatible with Optax optimizers~\citep{deepmind2020jax}.
However, as previously discussed, it may be necessary to skip optimizer updates if the gradients become infinite due to loss scaling. 
The function \texttt{optimizer\_update(model, optimizer, optimizer\_state, grads, grads\_finite)} implements this logic by updating the model only when gradients are finite.

Thus, instead of calling \texttt{optimizer.update} followed by \texttt{eqx.apply\_updates}, as in a typical Equinox training pipeline, one can simply use \texttt{mpx.optimizer\_update}.


\section{Example}
In this section, we provide an example to demonstrate the changes required in a training pipeline for mixed precision training via \mpx{}.

\subsection{Model Implementation}
\label{sec:eqxvsmpxmodelupdate}

% \begin{example}[t]
% \begin{pbox}[]{}
% \begin{minted}[linenos, breaklines, mathescape, fontsize=\footnotesize, xleftmargin=2em]{python}
% class MultiHeadAttentionBlock(eqx.Module):
%     dense_qs: eqx.nn.Linear
%     dense_ks: eqx.nn.Linear
%     dense_vs: eqx.nn.Linear
%     dense_o: eqx.nn.Linear
%     num_heads: int
%     layer_norm: eqx.nn.LayerNorm

%     def __init__(self, feature_dim, num_heads, key):
%         self.num_heads = num_heads
%         key, subkey = jax.random.split(key)
%         self.dense_qs = eqx.nn.Linear(
%             feature_dim, feature_dim, key=subkey)
%         # same for dense_ks, dense_vs, dense_o

%         self.layer_norm = eqx.nn.LayerNorm(feature_dim)

%     def attention(q, k, v):
%         attention_scores = q @ k.T / jnp.sqrt(q.shape[-1])
%         attention_scores = mpx.force_full_precision(
%             jax.nn.softmax, attention_scores.dtype)(attention_scores, axis=-1)
%         return attention_scores @ v
    
%     def __call__(self, inputs):
%         inputs_after_layernorm = jax.vmap(mpx.force_full_precision(
%             self.layer_norm, inputs.dtype))(inputs)
%         qs = jax.vmap(self.dense_qs)(inputs_after_layernorm)
%         qs = es.jax_einshape("n(hf)->hnf", qs, h=self.num_heads)
%         # same for ks and vs...

%         outputs = jax.vmap(self.attention, in_axes=(0, 0, 0))(qs, ks, vs)
%         outputs = es.jax_einshape("hnf->n(hf)", outputs)
%         outputs = jax.vmap(self.dense_o)(outputs)
%         outputs += inputs

%         return outputs
% \end{minted}
% \end{pbox}
% \caption{Implementation of multi-headed self-attention via MPX. \capt{For the largest part, the model can be implemented in common toolboxes (here, Equinox~\citep{kidger2021equinox}). Only critical function that should be implemented in full precision, like softmax or layernorm (due to mean and std calculations), have to be transformed via \mpx{}.}}
% \label{ex:model}
% \end{example}

For the largest part, the implementation of the model must not be changed.
As \mpx{} works with arbitrary PyTrees, every Toolbox that defines their model/parameters as PyTrees, like Flax~\citep{flax2020github} or Equinox~\citep{kidger2021equinox} can be used in conjunction with \mpx{} (Example~\ref{ex:model}). 
Only parts of the model that contain critical operations with a high risk of overflows, must be transformed to full precision via \texttt{mpx.force\_full\_precision}.
However, many jax and also equinox already implement many critical functions in a numerically stable way by casting to full precision if necessary, so that only few changes are necessary.

% \subsection{Model Updates}
% \label{sec:eqxvsmpxmodelupdate}
Model updates via \mpx{} closely resemble those performed with Equinox (Example~\ref{ex:update}). 
Instead of calling \texttt{eqx.filter\_grad}, \texttt{mpx.filter\_grad} is used. Similarly, \texttt{mpx.optimizer\_update} replaces invoking the Optax optimizer followed by a model update.
The remainder of the training step—such as loading batches, sharding the model and batch—remains unchanged. 
All these steps can be compiled using tools like \texttt{eqx.filter\_jit}.


\begin{example}[h]
    \centering
    \begin{subfigure}[t]{0.98\textwidth}
        \centering
        \begin{pbox}[]{}
            \begin{minted}[linenos, breaklines, mathescape, fontsize=\footnotesize, xleftmargin=2em]{python}
grads = eqx.filter_grad(loss)(model, batch)
updates, optimizer_state = optimizer.update(
    grads, optimizer_state, eqx.filter(model, eqx.is_array))
model = eqx.apply_updates(model, updates)
        \end{minted}
    \end{pbox}
        \caption{Full Precision via Equinox}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}[t]{0.98\textwidth}
        \centering
        \begin{pbox}[]{}
            \begin{minted}[linenos, breaklines, mathescape, fontsize=\footnotesize, xleftmargin=2em]{python}
loss_scaling, grads_finite, grads = mpx.filter_grad(loss, loss_scaling)(
    model, batch)
model, optimizer_state = mpx.optimizer_update(
    model, optimizer, optimizer_state, grads,grads_finite)
            \end{minted}
            \end{pbox}
            \caption{Mixed Precision via \mpx{}}
    \end{subfigure}
    \caption{Implementation of model updates via Equinox and \mpx{}. \mpx{} is designed such that its model update routine closely resembles the one of Equinox.}
    \label{ex:update}
\end{example}


\section{Evaluation}
We evaluate \mpx{} on two distinct machines: a mid-level desktop PC equipped with an AMD Ryzen 9 5950X, \qty{82}{\giga\byte} RAM, and an Nvidia GeForce RTX4070 GPU featuring \qty{12}{\giga\byte} VRAM; and a node from a high-performance cluster (CLAIX-2023) comprising Intel Xeon 8468 Sapphire CPUs, \qty{512}{\giga\byte} RAM, and four NVIDIA H100 GPUs connected via NVLink each with \qty{96}{\giga\byte} VRAM. 
On paper, the desktop PC's GPU maintains equivalent computation speeds for both half and full precision, whereas the H100 GPUs deliver double the speed for half precision compared to full precision.

We train vision transformers (ViT)~\citep{dosovitskiy2020image,steiner2021train}. On the desktop PC, the transformer features a size of 256 with residual blocks containing one hidden layer of 800 neurons. T
The cluster's transformer mirrors ViT-Base dimensions, with a feature size of 768 and residual blocks having one hidden layer of 3072 neurons.

The desktop PC trains the ViT on CIFAR100 while the high-performance cluster handles ImageNet1k, dividing each batch equally across GPUs using a data-parallel approach. We train without gradient accumulation, which could further optimize runtime and memory consumption.

We vary the number of batches and measure the time required for each training step across the entire dataset, excluding data loading times. 
RAM usage is measured by disabling automatic preallocation and setting "XLA\_PYTHON\_CLIENT\_PREALLOCATE" to "platform," minimizing JAX's VRAM consumption\footnote{\url{https://docs.jax.dev/en/latest/gpu_memory_allocation.html}}. 
For training speed assessment, both settings are turned off.
Due to technical reasons, we only measure VRAM consumption on the desktop PC as an accurate VRAM measurement was not possible on the high-performance cluster. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.9\linewidth}
    \begin{tikzpicture}
        \begin{axis}[
            width=0.8\textwidth,
            height=5cm,
            xlabel={Batch Size},
            ylabel={VRAM (\qty{}{\giga\byte})},
            legend pos=south east,
            grid=major,
            grid style={dashed,gray!30},
            xmin=0,
            ymin=0
        ]
        % First graph: Full Precision
        \addplot[
            color=blue,
            mark=o,
            thick
        ] coordinates {
            (128, 2.290)
            (256, 4.724)
            (512, 8.860)
        };
        \addlegendentry{Full Precision}

        % Second graph: Mixed Precision
        \addplot[
            color=red,
            mark=square*,
            thick,
            dashed
        ] coordinates {
            (128, 1.52)
            (256, 2.418)
            (512, 4.900)
            (768, 7.054)
            (1024, 9.222)
        };
        \addlegendentry{Mixed Precision}
        \end{axis}
    \end{tikzpicture}
    %\caption{Desktop Computer (RTX4070)}
    \end{subfigure}
    % \begin{subfigure}[t]{0.49\linewidth}
    % \begin{tikzpicture}
    %     \begin{axis}[
    %         width=0.8\textwidth,
    %         height=7cm,
    %         xlabel={Batch Size},
    %         ylabel={VRAM (\qty{}{\giga\byte})},
    %         legend pos=south east,
    %         grid=major,
    %         grid style={dashed,gray!30},
    %         xmin=0,
    %         ymin=0
    %     ]
    %     % First graph: Full Precision
    %     \addplot[
    %         color=blue,
    %         mark=o,
    %         thick
    %     ] coordinates {
    %         (128, 2.290)
    %         (256, 4.724)
    %         (512, 8.860)
    %     };
    %     \addlegendentry{Full Precision}

    %     % Second graph: Mixed Precision
    %     \addplot[
    %         color=red,
    %         mark=square*,
    %         thick,
    %         dashed
    %     ] coordinates {
    %         (128, 1.52)
    %         (256, 2.418)
    %         (512, 4.900)
    %         (768, 7.054)
    %         (1024, 9.222)
    %     };
    %     \addlegendentry{Mixed Precision}
    %     \end{axis}
    % \end{tikzpicture}
    % \caption{High-Performance Cluster ($4\times$ H100)}
    % \end{subfigure}
    \caption{Comparison of GPU VRAM consumed for full precision and mixed precision as a function of the number of batches on the desktop PC.}
    \label{fig:memory}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
    \begin{tikzpicture}
        \begin{axis}[
            width=0.8\textwidth,
            height=7cm,
            xlabel={Batch Size},
            ylabel={Training Time per Epoch (s)},
            legend pos=south east,
            grid=major,
            grid style={dashed,gray!30},
            xmin=0,
            ymin=0,
        ]
        % First graph: Full Precision
        \addplot[
            color=blue,
            mark=o,
            thick
        ] coordinates {
            (128, 19.7)
            (256, 18.8)
            (512, 19.7)
        };
        \addlegendentry{Full Precision}

        % Second graph: Mixed Precision
        \addplot[
            color=red,
            mark=square*,
            thick,
            dashed
        ] coordinates {
            (128, 12.6)
            (256, 12.0)
            (512, 11.4)
            (768, 11.6)
            (1024, 11.5)
        };
        \addlegendentry{Mixed Precision}
        \end{axis}
    \end{tikzpicture}
    \caption{Desktop Computer (RTX4070)}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
    \begin{tikzpicture}
        \begin{axis}[
            width=0.8\textwidth,
            height=7cm,
            xlabel={Batch Size},
            ylabel={Training Time per Epoch (s)},
            legend pos=south east,
            grid=major,
            grid style={dashed,gray!30},
            xmin=0,
            ymin=0,
        ]
        % First graph: Full Precision
        \addplot[
            color=blue,
            mark=o,
            thick
        ] coordinates {
            (512, 595.5616071701049)
            (1024, 452.3)
            (2048, 428.24)
            % (4096, 153.62)
            % (5120, 152)
            % (6144, 149.53)
            % (7168, 153.88)
        };
        \addlegendentry{Full Precision}

        % Second graph: Mixed Precision
        \addplot[
            color=red,
            mark=square*,
            thick,
            dashed
        ] coordinates {
            (512, 435.09)
            (1024, 325.61)
            (2048, 272.48)
            % (4096, 117.6)
            % (5120, 113.62)
            % (6144, 109.06)
            % (7168, 104.51)
            % (8192, 102.68)
        };
        \addlegendentry{Mixed Precision}
        \end{axis}
    \end{tikzpicture}
    \caption{High-Performance Cluster ($4\times$ H100)}
    \end{subfigure}
    \caption{Comparison of training step times for full precision and mixed precision as a function of the number of batches.}
    \label{fig:timing}
\end{figure}

Mixed precision training via \mpx{} reduces both RAM and processing times.
For the desktop PC, the RAM is reduced by factor $1.8\times$ and processing time by $1.7\times$.
The speedup is caused by reduced memory loading times as the RTX4070 streaming-multiprocessors feature no computing speedup for half precision.
The cluster features a similar behavior, where processing time is reduced by up to $1.57\times$.

These experiments demonstrate that mixed precision via \mpx{} allows to significantly speed up training.
Moreover, it significantly reduces VRAM usage.


\section{Conclusion}

\mpx{} offers an easy-to-use solution for mixed-precision training in JAX.
It is able to handle arbitrary PyTrees and provides a seamless integration with popular libraries like Equinox and Flax. 
At its core, \mpx{} allows casting functions to mixed precision and provides wrappers that allow to calculate gradients using mixed precision with dynamic loss scaling.
The evaluation demonstrates significant reductions in RAM usage and processing times. 
Overall, MPX presents a compelling toolbox for optimizing large-scale neural network training pipelines while maintaining flexibility and ease of use.


\section{Acknowledgements}
We thank Patrick Kidger for providing Equinox and Google DeepMind for JMP, which served as the foundation for this implementation.

We thank Lukas Wildberger for helpful discussions and suggestions.

The authors gratefully acknowledge the computing time provided to them at the NHR Center NHR4CES at RWTH Aachen University (project number p0021919). This is funded by the Federal Ministry of Education and Research, and the state governments participating on the basis of the resolutions of the GWK for national high performance computing at universities (\url{www.nhr-verein.de/unsere-partner}).

\bibliography{references}

\end{document}
